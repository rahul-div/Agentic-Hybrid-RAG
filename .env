# Database Configuration
DATABASE_URL=postgresql://neondb_owner:npg_SbL72CnhDWYR@ep-little-art-a1cz16pj-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require
# Example: postgresql://raguser:ragpass123@localhost:5432/agentic_rag_db
# Neon API Configuration (required for Phase 1)
NEON_API_KEY=napi_uvkea5r86xs3wi6fhcd6omvqf3fe2ivu0t45b2pxdiduzn7660v5nldoll3bj03n
NEON_DEFAULT_REGION=aws-us-east-1

# Catalog Database (use your existing Neon database for now)
CATALOG_DATABASE_URL=postgresql://neondb_owner:npg_SbL72CnhDWYR@ep-little-art-a1cz16pj-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require
# Also set these for compatibility with different components
CATALOG_DB_URL=postgresql://neondb_owner:npg_SbL72CnhDWYR@ep-little-art-a1cz16pj-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require
POSTGRES_URL=postgresql://neondb_owner:npg_SbL72CnhDWYR@ep-little-art-a1cz16pj-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require
# Pool Configuration
DATABASE_POOL_SIZE=10
DATABASE_MIN_CONNECTIONS=1
DATABASE_MAX_CONNECTIONS=20
# Google API key for Gemini (used for all AI tasks)
GOOGLE_API_KEY=AIzaSyCqesCHPPW7ouuql-iPOQRWVugW2erbf-0
GEMINI_API_KEY=AIzaSyCqesCHPPW7ouuql-iPOQRWVugW2erbf-0

# Neo4j (knowledge graph) connection details
NEO4J_URI=neo4j://127.0.0.1:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=Rahul4919@

# LLM Provider Configuration
# Set this to gemini for all AI tasks
LLM_PROVIDER=gemini

# Base URL for Gemini API
LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta

# API Key for LLM provider (Gemini)
LLM_API_KEY=AIzaSyCY7tVMwji2F60QmexMkRM63r1K_CgeW-I

# The LLM you want to use for the agents
# Options: gemini-2.0-flash-exp, gemini-2.0-flash, gemini-2.5-flash
LLM_CHOICE=gemini-2.0-flash-thinking-exp-1219

# Model choice for your agent.py (matches the MODEL_CHOICE in your code)
MODEL_CHOICE=gemini-2.0-flash-thinking-exp-1219

# Embedding Provider Configuration - CHANGED TO GEMINI
EMBEDDING_PROVIDER=gemini

# Base URL for embedding models - CHANGED TO GEMINI
EMBEDDING_BASE_URL=https://generativelanguage.googleapis.com/v1beta

# API Key for embedding provider - CHANGED TO GEMINI
EMBEDDING_API_KEY=AIzaSyCY7tVMwji2F60QmexMkRM63r1K_CgeW-I

# The embedding model you want to use for RAG - CHANGED TO GEMINI
# Google's text embedding model
EMBEDDING_MODEL=embedding-001

# Ingestion-specific LLM (using Gemini for processing)
INGESTION_LLM_CHOICE=gemini-2.0-flash-thinking-exp-1219

# Application Configuration
APP_ENV=development
LOG_LEVEL=INFO
APP_HOST=0.0.0.0
APP_PORT=8058

# Chunking Configuration (optimized for Graphiti token limits)
CHUNK_SIZE=800
CHUNK_OVERLAP=150
MAX_CHUNK_SIZE=1500

# Vector Search Configuration - CHANGED FOR GEMINI EMBEDDINGS
VECTOR_DIMENSION=768  # For Google embedding-001 model (was 1536 for OpenAI)
MAX_SEARCH_RESULTS=10

# Session Configuration
SESSION_TIMEOUT_MINUTES=60
MAX_MESSAGES_PER_SESSION=100

# Rate Limiting
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_WINDOW_SECONDS=60

# File Processing
MAX_FILE_SIZE_MB=10
ALLOWED_FILE_EXTENSIONS=.md,.txt

# Debug Configuration
DEBUG_MODE=false
ENABLE_PROFILING=false

# Gemini-specific model configurations for different components
GEMINI_CHAT_MODEL=gemini-2.0-flash-thinking-exp-1219
GEMINI_GRAPHITI_MODEL=gemini-2.0-flash-thinking-exp-1219
GEMINI_EMBEDDING_MODEL=embedding-001
GEMINI_RERANKER_MODEL=gemini-2.0-flash-thinking-exp-1219
# New Onyx Integration
ONYX_API_KEY=on_tenant_f77e9e8b-75c7-4dc4-bea4-3fed8bb60a0f.w6dF-qxJmV4A1RVXRTnSBY6GqRIyf23yWldezF5vJjjZo9k7jn_ke3wI-quqOl92ydF19Ke3gUyQnY7KL7GXKZFaqncbpDnkfsgQZ7hmcIS6BPwIZ1pufJTLVSKw-E8DO5LT-_K2UcmF9ydAdHSpbpiRFEehMH5uRPJdMtJxJB-GNxofACNpICHXuYu7H9riO3YsriZZ5U45SL_L7O33PgOWG7gtpmWAtYxnaJ9CRVzWc9RKHZqfE1hMHGUnxwsN
ONYX_BASE_URL=https://cloud.onyx.app
ONYX_TIMEOUT=30
ONYX_DOCUMENT_SET_ID=default
